{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this model i have used The Complete Works of William Shakespeare by William Shakespeare plain text UTF-8\n",
        "\n",
        "to download this text data go to https://www.gutenberg.org/ebooks/100 and find plain text UTF-8 and save it as .txt and upload it into google colab environment\n",
        "\n",
        "as there was time constrain i used smaller set from the dataset and the accuracy was low but given enough time i can fine tune the model and ruduce the training time and increace the model accuracy  "
      ],
      "metadata": {
        "id": "kYE62uGuC3O8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vI3OTCHb6Kv6",
        "outputId": "55a1cec9-b9e8-4860-e17e-d5739f55c299"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.3.6)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m103.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wheel, werkzeug, tensorflow-io-gcs-filesystem, tensorboard-data-server, google-pasta, tensorboard, astunparse, tensorflow\n",
            "Successfully installed astunparse-1.6.3 flatbuffers-25.2.10 google-pasta-0.2.0 libclang-18.1.1 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 werkzeug-3.1.3 wheel-0.45.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wlN7Lb8CC1hO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "y67in-Vv5vfz"
      },
      "outputs": [],
      "source": [
        "# Import statements and parameters\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FILE_PATH = 'shakespeare.txt'\n",
        "MAX_TEXT_LENGTH = 50000\n",
        "\n",
        "try:\n",
        "    with open(FILE_PATH, 'r', encoding='utf-8') as f:\n",
        "        text_data = f.read(MAX_TEXT_LENGTH) # Load only a part of the text\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{FILE_PATH}' was not found. Please upload it to your Colab session.\")\n",
        "    text_data = \"This is a dummy text if the file is not found. Please upload your dataset.\"\n",
        "\n",
        "# Inspect the data (first 1000 characters) ---\n",
        "if text_data:\n",
        "    print(\"--- First 1000 characters of the dataset ---\")\n",
        "    print(text_data[:1000])\n",
        "    print(f\"\\n--- Total characters in dataset: {len(text_data)} ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqHY-uu66psU",
        "outputId": "9de913b9-0adb-4cc3-e0a7-0bc8b03eefef"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- First 1000 characters of the dataset ---\n",
            "﻿The Project Gutenberg eBook of The Complete Works of William Shakespeare\n",
            "    \n",
            "This ebook is for the use of anyone anywhere in the United States and\n",
            "most other parts of the world at no cost and with almost no restrictions\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\n",
            "of the Project Gutenberg License included with this ebook or online\n",
            "at www.gutenberg.org. If you are not located in the United States,\n",
            "you will have to check the laws of the country where you are located\n",
            "before using this eBook.\n",
            "\n",
            "Title: The Complete Works of William Shakespeare\n",
            "\n",
            "Author: William Shakespeare\n",
            "\n",
            "Release date: January 1, 1994 [eBook #100]\n",
            "                Most recently updated: October 29, 2024\n",
            "\n",
            "Language: English\n",
            "\n",
            "\n",
            "\n",
            "*** START OF THE PROJECT GUTENBERG EBOOK THE COMPLETE WORKS OF WILLIAM SHAKESPEARE ***\n",
            "The Complete Works of William Shakespeare\n",
            "\n",
            "by William Shakespeare\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                    Contents\n",
            "\n",
            "    THE SONNETS\n",
            "    ALL’S WELL THAT ENDS WELL\n",
            "    THE TRAGEDY OF ANTONY AND CLEOPATRA\n",
            "    \n",
            "\n",
            "--- Total characters in dataset: 50000 ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the text\n",
        "def preprocess_text(text):\n",
        "    processed_text = text.lower()\n",
        "    processed_text = re.sub(r'^\\s*\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK.*? \\*\\*\\*\\s*', '', processed_text, flags=re.IGNORECASE | re.DOTALL)\n",
        "    processed_text = re.sub(r'\\s*\\*\\*\\* END OF THIS PROJECT GUTENBERG EBOOK.*? \\*\\*\\*.*$', '', processed_text, flags=re.IGNORECASE | re.DOTALL)\n",
        "    processed_text = re.sub(r'produced by .*?\\n', '', processed_text, flags=re.IGNORECASE)\n",
        "    processed_text = re.sub(r'\\[illustration.*?\\]', '', processed_text, flags=re.IGNORECASE)\n",
        "    allowed_punctuation = '.\\n'\n",
        "    unwanted_chars_pattern = r\"[^a-z0-9\\s\" + re.escape(allowed_punctuation) + r\"]\"\n",
        "    processed_text = re.sub(unwanted_chars_pattern, ' ', processed_text)\n",
        "    processed_text = re.sub(r'\\s+', ' ', processed_text).strip()\n",
        "    return processed_text\n",
        "\n",
        "if 'text_data' in locals() and text_data:\n",
        "    cleaned_text = preprocess_text(text_data)\n",
        "    print(\"--- First 500 characters of cleaned text ---\")\n",
        "    print(cleaned_text[:500])\n",
        "    print(f\"\\n--- Total characters in cleaned dataset: {len(cleaned_text)} ---\")\n",
        "else:\n",
        "    print(\"Text data not loaded. Please ensure the file is uploaded and Cell 2 ran successfully.\")\n",
        "    cleaned_text = \"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1Bhjiq4-RpH",
        "outputId": "e9329693-8b94-4ce1-bc0b-9f028019dc9f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- First 500 characters of cleaned text ---\n",
            "the project gutenberg ebook of the complete works of william shakespeare this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions whatsoever. you may copy it give it away or re use it under the terms of the project gutenberg license included with this ebook or online at www.gutenberg.org. if you are not located in the united states you will have to check the laws of the country where you are located before usin\n",
            "\n",
            "--- Total characters in cleaned dataset: 46629 ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization and sequence generation\n",
        "if 'cleaned_text' in locals() and cleaned_text:\n",
        "    chars = sorted(list(set(cleaned_text)))\n",
        "    char_to_int = {char: i for i, char in enumerate(chars)}\n",
        "    int_to_char = {i: char for i, char in enumerate(chars)}\n",
        "\n",
        "    n_chars = len(cleaned_text)\n",
        "    n_vocab = len(chars)\n",
        "\n",
        "    print(f\"--- Total unique characters (vocab size): {n_vocab} ---\")\n",
        "    if n_vocab > 0:\n",
        "        print(f\"--- Character mapping (first 10): {list(char_to_int.items())[:min(10, n_vocab)]} ---\")\n",
        "    else:\n",
        "        print(\"--- No characters found in cleaned_text. Vocabulary is empty. ---\")\n",
        "\n",
        "    seq_length = 50  # Shorter sequence length for faster training\n",
        "    step = 1\n",
        "\n",
        "    input_sequences = []\n",
        "    output_chars = []\n",
        "\n",
        "    if n_vocab > 0 and n_chars > seq_length:\n",
        "        for i in range(0, n_chars - seq_length, step):\n",
        "            current_seq_text = cleaned_text[i : i + seq_length]\n",
        "            next_char_text = cleaned_text[i + seq_length]\n",
        "            input_sequences.append([char_to_int[char] for char in current_seq_text])\n",
        "            output_chars.append(char_to_int[next_char_text])\n",
        "\n",
        "        n_patterns = len(input_sequences)\n",
        "        print(f\"--- Total input-output patterns: {n_patterns} ---\")\n",
        "\n",
        "        if n_patterns > 0:\n",
        "            print(\"--- Example Input Sequence (first actual sequence): ---\")\n",
        "            print(\"Text:\", cleaned_text[0 : seq_length])\n",
        "            print(\"Ints:\", input_sequences[0])\n",
        "            print(\"--- Example Output Character (for first actual sequence): ---\")\n",
        "            print(\"Text:\", cleaned_text[seq_length])\n",
        "            print(\"Int:\", output_chars[0])\n",
        "        else:\n",
        "            print(\"No patterns generated. Check seq_length or cleaned_text content.\")\n",
        "    elif n_vocab == 0:\n",
        "        print(\"Vocabulary size is 0. Cannot create sequences.\")\n",
        "        n_patterns = 0\n",
        "    else:\n",
        "        print(f\"Cleaned text length ({n_chars}) is not greater than sequence length ({seq_length}). Cannot create sequences.\")\n",
        "        n_patterns = 0\n",
        "else:\n",
        "    print(\"Cleaned text is empty or not defined. Please check previous cells.\")\n",
        "    n_patterns = 0\n",
        "    input_sequences = []\n",
        "    output_chars = []\n",
        "    n_vocab = 0\n",
        "    seq_length = 100\n",
        "    char_to_int = {}\n",
        "    int_to_char = {}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tbOcdSn-UbO",
        "outputId": "7e42a6f5-b483-40eb-c4ce-51ec9e4e4a11"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Total unique characters (vocab size): 38 ---\n",
            "--- Character mapping (first 10): [(' ', 0), ('.', 1), ('0', 2), ('1', 3), ('2', 4), ('3', 5), ('4', 6), ('5', 7), ('6', 8), ('7', 9)] ---\n",
            "--- Total input-output patterns: 46579 ---\n",
            "--- Example Input Sequence (first actual sequence): ---\n",
            "Text: the project gutenberg ebook of the complete works \n",
            "Ints: [31, 19, 16, 0, 27, 29, 26, 21, 16, 14, 31, 0, 18, 32, 31, 16, 25, 13, 16, 29, 18, 0, 16, 13, 26, 26, 22, 0, 26, 17, 0, 31, 19, 16, 0, 14, 26, 24, 27, 23, 16, 31, 16, 0, 34, 26, 29, 22, 30, 0]\n",
            "--- Example Output Character (for first actual sequence): ---\n",
            "Text: o\n",
            "Int: 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for the Keras embedding model\n",
        "if ('n_patterns' in locals() and n_patterns > 0 and\n",
        "    'input_sequences' in locals() and 'output_chars' in locals() and\n",
        "    'n_vocab' in locals() and n_vocab > 0 and 'seq_length' in locals()):\n",
        "\n",
        "    X_train = np.array(input_sequences)\n",
        "    y_train = to_categorical(output_chars, num_classes=n_vocab)\n",
        "\n",
        "    print(\"--- Data Shapes for Model Training ---\")\n",
        "    print(\"X_train shape:\", X_train.shape)\n",
        "    print(\"y_train shape:\", y_train.shape)\n",
        "\n",
        "else:\n",
        "    print(\"Skipping model data preparation: Not enough patterns or necessary variables are missing.\")\n",
        "    print(\"Ensure Cell 4 (Tokenization and Sequence Generation) ran successfully.\")\n",
        "    X_train = None\n",
        "    y_train = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGSpzc79-Xr2",
        "outputId": "a27b9a08-5235-4f6f-bbff-742eb264d05b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Data Shapes for Model Training ---\n",
            "X_train shape: (46579, 50)\n",
            "y_train shape: (46579, 38)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model design and compilation\n",
        "if 'n_vocab' in locals() and 'seq_length' in locals():\n",
        "    embedding_dim = 50 # Reduced embedding dimension\n",
        "    lstm_units = 128 # Reduced LSTM units\n",
        "\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=n_vocab, output_dim=embedding_dim, input_shape=(seq_length,)),\n",
        "        LSTM(lstm_units),\n",
        "        Dropout(0.2),\n",
        "        Dense(n_vocab, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "    print(\"\\n--- Model Summary ---\")\n",
        "    model.summary()\n",
        "\n",
        "else:\n",
        "    print(\"Skipping model design and compilation: 'n_vocab' or 'seq_length' not defined.\")\n",
        "    model = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "sZQyYX6o-bT9",
        "outputId": "a9845f9b-ca75-49cb-ca8a-91483de06293"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Model Summary ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │         \u001b[38;5;34m1,900\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m91,648\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m38\u001b[0m)             │         \u001b[38;5;34m4,902\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,900</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">91,648</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,902</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m98,450\u001b[0m (384.57 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">98,450</span> (384.57 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m98,450\u001b[0m (384.57 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">98,450</span> (384.57 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model training\n",
        "if 'model' in locals() and model is not None and \\\n",
        "    'X_train' in locals() and X_train is not None and \\\n",
        "    'y_train' in locals() and y_train is not None:\n",
        "    print(\"--- Starting Model Training ---\")\n",
        "    epochs = 8 # Reduced number of epochs for faster training\n",
        "    batch_size = 128 # Adjust based on memory and dataset size\n",
        "\n",
        "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size) # Removed validation_split for faster training\n",
        "    print(\"--- Model Training Complete ---\")\n",
        "\n",
        "    # You can then save the model, plot history, and generate text\n",
        "    model.save('shakespeare_char_lstm_model.keras')\n",
        "else:\n",
        "    print(\"Model, X_train, or y_train not available. Cannot start training.\")\n",
        "    print(\"Please ensure Cells 1-6 ran successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttCX4EP7-eV7",
        "outputId": "4758e3b9-053d-4152-a6eb-a8d7609b2ed5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Model Training ---\n",
            "Epoch 1/8\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 156ms/step - accuracy: 0.4545 - loss: 1.8324\n",
            "Epoch 2/8\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 155ms/step - accuracy: 0.4575 - loss: 1.7971\n",
            "Epoch 3/8\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 155ms/step - accuracy: 0.4731 - loss: 1.7530\n",
            "Epoch 4/8\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 156ms/step - accuracy: 0.4818 - loss: 1.7183\n",
            "Epoch 5/8\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 155ms/step - accuracy: 0.4834 - loss: 1.6904\n",
            "Epoch 6/8\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 155ms/step - accuracy: 0.4931 - loss: 1.6639\n",
            "Epoch 7/8\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 156ms/step - accuracy: 0.4985 - loss: 1.6456\n",
            "Epoch 8/8\n",
            "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 155ms/step - accuracy: 0.5056 - loss: 1.6143\n",
            "--- Model Training Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Text Generation\n",
        "\n",
        "if 'model' in locals() and model is not None and \\\n",
        "   'int_to_char' in locals() and 'char_to_int' in locals() and \\\n",
        "   'input_sequences' in locals() and len(input_sequences) > 0 and \\\n",
        "   'seq_length' in locals() and 'n_vocab' in locals():\n",
        "\n",
        "    print(\"--- Text Generation ---\")\n",
        "\n",
        "    # Select a random seed sequence\n",
        "    start_index = np.random.randint(0, len(input_sequences) - 1)\n",
        "    seed_sequence = input_sequences[start_index]\n",
        "    generated_text = [int_to_char[index] for index in seed_sequence]\n",
        "    print(\"--- Generating text with seed: ---\\n\", \"\".join(generated_text))\n",
        "\n",
        "    # Parameters for text generation\n",
        "    num_chars_to_generate = 100\n",
        "    print(\"\\n--- Generating {} characters... ---\".format(num_chars_to_generate))\n",
        "\n",
        "    # Generate text\n",
        "    for _ in range(num_chars_to_generate):\n",
        "       n\n",
        "        input_array = np.reshape(seed_sequence, (1, seq_length))\n",
        "\n",
        "        # Make a prediction\n",
        "        prediction = model.predict(input_array, verbose=0)\n",
        "        predicted_index = np.argmax(prediction)\n",
        "        predicted_char = int_to_char[predicted_index]\n",
        "\n",
        "        # Append the predicted character to the generated text and update the seed\n",
        "        generated_text.append(predicted_char)\n",
        "        seed_sequence.append(predicted_index)\n",
        "        seed_sequence = seed_sequence[1:] # Remove the first character to maintain sequence length\n",
        "\n",
        "    print(\"\\n--- Generated Text: ---\")\n",
        "    print(\"\".join(generated_text))\n",
        "\n",
        "else:\n",
        "    print(\"Model or necessary variables not found. Please ensure all previous cells have been run successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WM8FZlKwBp_9",
        "outputId": "dbd9c682-d357-480b-9029-8f1270ed6d9d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Text Generation ---\n",
            "--- Generating text with seed: ---\n",
            " ost common grow. 70 that thou art blamed shall not\n",
            "\n",
            "--- Generating 100 characters... ---\n",
            "\n",
            "--- Generated Text: ---\n",
            "ost common grow. 70 that thou art blamed shall not the self the self the self the self the self the self the self the self the self the self the self \n"
          ]
        }
      ]
    }
  ]
}